{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74600d4c",
   "metadata": {},
   "source": [
    "# Edge AI Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f536ae1",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This document contains the code and the instructions for our EclipseCON 2022 Talk: \"_How to Train Your Dragon and Its Friends: AI on the Edge with Eclipse Kura&trade;_\"\n",
    "\n",
    "In this example scenario we will collect the data provided by a [Raspberry Pi Sense HAT](https://www.raspberrypi.com/products/sense-hat/) using [Eclipse Kura&trade;](https://www.eclipse.org/kura/) and upload them to a [Eclipse Kapua&trade;](https://www.eclipse.org/kapua/) instance. We will then download this data and train an AI-based anomaly detector using [TensorFlow](https://www.tensorflow.org/). Finally we will deploy the trained anomaly detector model leveraging [Nvidia Triton&trade; Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server) and Eclipse Kura&trade; integration.\n",
    "\n",
    "![image1.png](imgs/img1.png)\n",
    "\n",
    "We'll subdivide this example scenario in three main sections:\n",
    "1. **Data collection**: in this section we'll discuss how to retrieve training data from the field leveraging Eclipse Kura&trade; and Eclipse Kapua&trade;\n",
    "2. **Model building and training**: we'll further divide this section in three subsections:\n",
    "    - *Data processing*: where we'll show how to explore our training data and manipulate them to make them suitable for training (feature selection, scaling and dataset splitting). This will provide us with the \"_Preprocessing_\" stage of the resulting AI data-processing pipeline\n",
    "    - *Model training*: where we'll discuss how we can create a simple Autoencoder in Tensorflow Keras and how to train it. This will provide us with the \"_Inference_\" stage of the AI pipeline\n",
    "    - *Model evaluation*: where we'll cover how can we extract the high level data from the model output and ensure the model was trained correctly. This will provide us with the \"_Postprocessing_\" stage of the AI pipeline\n",
    "3. **Model deployment**: finally we will convert the model to make it suitable for running on Eclipse Kura&trade; and Nvidia Triton&trade; and deploy it on the edge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de578854",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1c0157",
   "metadata": {},
   "source": [
    "## Data collection\n",
    "\n",
    "### Overview\n",
    "\n",
    "In this setup we'll leverage Eclipe Kura&trade; and Kapua&trade; for retrieving data from a [Raspberry Pi Sense HAT](https://www.raspberrypi.com/products/sense-hat/) and upload them to the cloud.\n",
    "\n",
    "The Sense HAT is an add-on board for Raspberry Pi which provides an 8×8 RGB LED matrix, a five-button joystick and includes the following sensors:\n",
    "\n",
    "- Gyroscope\n",
    "- Accelerometer\n",
    "- Magnetometer\n",
    "- Temperature\n",
    "- Barometric pressure\n",
    "- Humidity\n",
    "\n",
    "![image2.png](imgs/img2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ef33cb-9a93-4278-a01e-b07979300308",
   "metadata": {},
   "source": [
    "### Kura&trade; installation\n",
    "\n",
    "**Requirement**: A Raspberry Pi 3/4 running the latest version of Raspberry Pi OS 64 bit.\n",
    "\n",
    "To make everything work on the Raspberry Pi we need to use the `develop` version of the `raspberry-pi-ubuntu-20-nn` Kura installer (yes, I know we're installing the Ubuntu package on the Raspberry Pi OS but bear with me...) . You can do so by downloading the repo and [building locally](https://github.com/eclipse/kura#build-kura) or by downloading a pre-built installer from the [Kura CI artifacts](https://ci.eclipse.org/kura/).\n",
    "\n",
    "Copy the resulting file `kura_<version>_raspberry-pi-ubuntu-20_installer-nn.deb` on the target device.\n",
    "\n",
    "On the target device run the following commands:\n",
    "\n",
    "```bash\n",
    "sudo apt-get install -y wget apt-transport-https gnupg\n",
    "```\n",
    "```bash\n",
    "sudo wget -O - https://packages.adoptium.net/artifactory/api/gpg/key/public | sudo apt-key add -\n",
    "```\n",
    "```bash\n",
    "sudo echo \"deb https://packages.adoptium.net/artifactory/deb $(awk -F= '/^VERSION_CODENAME/{print$2}' /etc/os-release) main\" | sudo tee /etc/apt/sources.list.d/adoptium.list\n",
    "```\n",
    "```bash\n",
    "sudo apt-get update && sudo apt-get install temurin-8-jdk chrony\n",
    "```\n",
    "\n",
    "Finally install Kura with:\n",
    "```bash\n",
    "sudo apt install ./kura_<version>_raspberry-pi-ubuntu-20_installer-nn.deb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22d130b",
   "metadata": {},
   "source": [
    "### Cloud connection\n",
    "\n",
    "After setting up an Eclipse Kura&trade; instance on the Raspberry Pi we'll need to connect it to an [Eclipse Kapua&trade;](https://www.eclipse.org/kapua/) instance.\n",
    "\n",
    "![image3.png](imgs/img3.png)\n",
    "\n",
    "An excellent tutorial on how to deploy a Kapua&trade; instance using Docker is available [in the official repository](https://github.com/eclipse/kapua/blob/develop/deployment/docker/README.md). For the purpose of this tutorial we'll assume a Kapua&trade; instance is already running and is available for connection from Kura&trade;\n",
    "\n",
    "After setting up the Kapua&trade; instance you can refer to the [official Kura&trade; documentation](https://eclipse.github.io/kura/docs-develop/cloud-platform/kura-kapua/) for connecting the Raspberry Pi to the Kapua&trade; instance. For the remaining of this tutorial we'll assume a connection with the Kapua&trade; was correctly established.\n",
    "\n",
    "![image4.png](imgs/img4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a363da-b11e-4b99-b4af-f32265944d91",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Data publisher\n",
    "\n",
    "To publish the collected data on the Cloud we'll need to create a new [Cloud Publisher](https://eclipse.github.io/kura/docs-release-5.2/cloud-api/overview/#cloudpublisher) through the Kura&trade; web interface. Go to \"Cloud Connections\" and press \"New Pub/Sub\", in the example below we'll call our new publisher `KapuaSenseHatPublisher`.\n",
    "\n",
    "![image5.png](imgs/img5.png)\n",
    "\n",
    "To keep things clean we'll create a new topic called `SenseHat`. To do so we'll move to the `KapuaSenseHatPublisher` configuration and we'll update the `Application Topic` field to `A1/SenseHat`\n",
    "\n",
    "![image6.png](imgs/img6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8220ccde-f812-498a-9228-6c2b65b805ed",
   "metadata": {},
   "source": [
    "### SenseHat driver\n",
    "\n",
    "Kura&trade; provides a driver that allows to interact to a RaspberryPi SenseHat device using [Kura Driver, Asset and Wires frameworks](https://eclipse.github.io/kura/docs-develop/connect-field-devices/driver-and-assets/).\n",
    "\n",
    "From the Kura&trade; documentation:\n",
    "\n",
    "> Eclipse Kura introduces a model based on the concepts of Drivers and Assets to simplify the communication with the field devices attached to a gateway.\n",
    ">\n",
    "> A **Driver** encapsulates the communication protocol and its configuration parameters, dealing with the low-level characteristics of the field protocol. It opens, closes and performs the communication with the end field device. It also exposes field protocol specific information that can be used by upper levels of abstraction to simplify the interaction with the end devices.\n",
    ">\n",
    "> An **Asset** is a logical representation of a field device, described by a list of **Channels**. The Asset uses a specific Driver instance to communicate with the underlying device and it models a generic device resource as a Channel. A register in a PLC or a GATT Characteristic in a Bluetooth device are examples of Channels. In this way, each Asset has multiple Channels for reading and writing data from/to an Industrial Device.\n",
    "\n",
    "The Kura Sense Hat driver requires a few changes on the Raspberry Pi:\n",
    "- Configured SenseHat: see [SenseHat documentation](https://www.raspberrypi.com/documentation/accessories/sense-hat.html)\n",
    "- I2C interface should be unlocked using `sudo raspi-config`\n",
    "\n",
    "As others Drivers supported by Kura, it is distributed as a deployment package on the Eclipse Marketplace. It consists of two packages:\n",
    "- [SenseHat Example Driver](https://marketplace.eclipse.org/content/sensehat-example-driver-eclipse-kura-4xy).\n",
    "- [SenseHat Support Library](https://marketplace.eclipse.org/content/sensehat-support-library-bundle-eclipse-kura-45)\n",
    "\n",
    "We need to install both. Complete installation instructions are available [here](https://eclipse.github.io/kura/docs-develop/connect-field-devices/sensehat-driver/).\n",
    "\n",
    "#### Driver configuration\n",
    "\n",
    "We now need to configure the driver to access the sensors on the SenseHat. Move to the \"Driver and Assets\" section of the web UI and create a new driver. We'll call it `driver-sensehat`.\n",
    "\n",
    "![image7.png](imgs/img7.png)\n",
    "\n",
    "Then add a new Asset (which we'll call `asset-sensehat`) to this driver and configure it as per the screenshots below. We'll need a Channel for every sensor we want to access.\n",
    "\n",
    "![image8.png](imgs/img8.png)\n",
    "\n",
    "![image9.png](imgs/img9.png)\n",
    "\n",
    "Refer to the following table for the driver parameters:\n",
    "\n",
    "| name       | type | value.type | resource                  |\n",
    "|------------|------|------------|---------------------------|\n",
    "| ACC_X      | READ | FLOAT      | ACCELERATION_X            |\n",
    "| ACC_Y      | READ | FLOAT      | ACCELERATION_Y            |\n",
    "| ACC_Z      | READ | FLOAT      | ACCELERATION_Z            |\n",
    "| GYRO_X     | READ | FLOAT      | GYROSCOPE_X               |\n",
    "| GYRO_Y     | READ | FLOAT      | GYROSCOPE_Y               |\n",
    "| GYRO_Z     | READ | FLOAT      | GYROSCOPE_Z               |\n",
    "| HUMIDITY   | READ | FLOAT      | HUMIDITY                  |\n",
    "| PRESSURE   | READ | FLOAT      | PRESSURE                  |\n",
    "| TEMP_HUM   | READ | FLOAT      | TEMPERATURE_FROM_HUMIDITY |\n",
    "| TEMP_PRESS | READ | FLOAT      | TEMPERATURE_FROM_PRESSURE |\n",
    "\n",
    "After correctly configuring it you should see the data in the \"Data\" page of the UI.\n",
    "\n",
    "![image10.png](imgs/img10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9f7342-03d2-4f9a-b886-89fc97ce04a7",
   "metadata": {},
   "source": [
    "### Wire graph\n",
    "\n",
    "Now that we have our Driver and Cloud Publisher ready we can put everything together with a [Kura Wire Graph](https://eclipse.github.io/kura/docs-develop/kura-wires/introduction/).\n",
    "\n",
    "From Kura&trade; documentation:\n",
    "\n",
    "> The Kura&trade; Wires feature aims to simplify the development of IoT Edge Computing Applications leveraging reusable configurable components that can be wired together and which, eventually, allows configurable cooperation between these components.\n",
    ">\n",
    "> In the dataflow programming model, the application logic is expressed as a directed graph (flow) where each node can have inputs, outputs, and independent processing units. There are nodes that only produce outputs and ones that only consume inputs, which usually represent the start and the end of the flow. The inner-graph nodes process the inputs and produce outputs for downstream nodes. The processing unit of a node executes independently and does not affect the execution of other nodes. Thus, the nodes are highly reusable and portable.\n",
    "\n",
    "Move to the \"Wire Graph\" section of the UI. We'll need a graph with three components:\n",
    "- A `Timer` which will dictate the sample rate at which we will collect data coming from the Sense Hat\n",
    "- A `WireAsset` for the Sense Hat driver asset\n",
    "- A `Publisher` for the Kapua publisher we created before.\n",
    "\n",
    "The resulting Wire Graph will look like this:\n",
    "\n",
    "![image11.png](imgs/img11.png)\n",
    "\n",
    "#### Timer\n",
    "\n",
    "Configure the timer such that it will poll the SenseHat each second, this can be done by setting the `simple.interval` to `1`.\n",
    "\n",
    "![image12.png](imgs/img12.png)\n",
    "\n",
    "#### WireAsset\n",
    "\n",
    "Select the `driver-sensehat` when creating the WireAsset. No further configuration is needed for this component.\n",
    "\n",
    "![image13.png](imgs/img13.png)\n",
    "\n",
    "#### Publisher\n",
    "\n",
    "Create a \"Publisher\" Wire component and select the `KapuaSensehatPublisher` from the target filter.\n",
    "\n",
    "![image14.png](imgs/img14.png)\n",
    "\n",
    "Don't forget to press \"Apply\" to start the Wire Graph!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86110baa-a6ac-4473-aaa2-eaa4e5883863",
   "metadata": {},
   "source": [
    "### Collect the data\n",
    "\n",
    "At this point you should see data coming from the Rasperry Pi from the Kapua&trade; console under the `SenseHat` topic.\n",
    "\n",
    "![image15.png](imgs/img15.png)\n",
    "\n",
    "You can download the `.csv` file directly from the console using the \"_Export to CSV_\" button."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264c9984",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d243fa",
   "metadata": {},
   "source": [
    "## Model building and training\n",
    "\n",
    "### Overview\n",
    "\n",
    "We will now use the data collected in the previous section to train an artificial neural network-based Anomaly Detector of our design. To this end we will use an Autoencoder model. To understand why we choose such model we need to understand how it works. From [Wikipedia](https://en.wikipedia.org/wiki/Autoencoder):\n",
    "\n",
    "> An autoencoder is a type of artificial neural network used to learn efficient codings of unlabeled data (unsupervised learning). The encoding is validated and refined by attempting to regenerate the input from the encoding. The autoencoder learns a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore insignificant data (“noise”).\n",
    "\n",
    "> Another application for autoencoders is **anomaly detection**. By learning to replicate the most salient features in the training data [...] the model is encouraged to learn to precisely reproduce the most frequently observed characteristics. When facing anomalies, the model should worsen its reconstruction performance. In most cases, only data with normal instances are used to train the autoencoder; in others, the frequency of anomalies is small compared to the observation set so that its contribution to the learned representation could be ignored. **After training, the autoencoder will accurately reconstruct \"normal\" data, while failing to do so with unfamiliar anomalous data**. Reconstruction error (the error between the original data and its low dimensional reconstruction) is used as an anomaly score to detect anomalies\n",
    "\n",
    "In simple terms: \n",
    "- The Autoencoder is a artificial neural network model that learns how to reconstruct the input data at the output. \n",
    "- If trained on \"normal\" data, it learns to recontruct **only normal data** and fails to reconstruct anomalies.\n",
    "- We can detect anomalies by computing the reconstruction error of the Autoencoder. If the error is above a certain threshold (which we will decide) the input sample is an anomaly.\n",
    "\n",
    "Why did we choose this approach over others?\n",
    "- The Autoencoder falls in the \"[Unsupervised Learning](https://en.wikipedia.org/wiki/Unsupervised_learning)\" category: it doesn't need labeled data to be trained i.e. we don't need to go through all the dataset and manually label the samples as \"normal\" or \"anomaly\" ([Supervised Learning](https://en.wikipedia.org/wiki/Supervised_learning)).\n",
    "- Simpler data collection: we just need to provide it with the \"normal\" data. We don't need to artificially generate anomalies to train it on them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a09146",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "\n",
    "We can now work on our `.csv` file downloaded from Kapua. For demonstration purposes an already available dataset is provided within this repository.\n",
    "\n",
    "If you're running this notebook through Google Colab you'll need to download the dataset running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2abcb89-b09b-4b28-87e8-77265a01d400",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/mattdibi/eclipsecon-edgeAI-talk/master/notebook/train-data-raw.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e652ecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls *.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0e401f",
   "metadata": {},
   "source": [
    "Let's start taking a look at the content of this dataset, we'll use [pandas](https://pandas.pydata.org/) (Python Data Analysis library) for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95543e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_data = pd.read_csv(\"./train-data-raw.csv\")\n",
    "\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab22fae",
   "metadata": {},
   "source": [
    "#### Feature selection\n",
    "\n",
    "As you might notice there's some information in the dataset we don't care about and are not meaningful for our application:\n",
    "- `ID`\n",
    "- The various `timestamps`\n",
    "- `assetName` which doesn't change\n",
    "\n",
    "Then we can remove them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba782ee7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = ['ACC_Y', 'ACC_X', 'ACC_Z',\n",
    "            'PRESSURE', 'TEMP_PRESS', 'TEMP_HUM',\n",
    "            'HUMIDITY', 'GYRO_X', 'GYRO_Y', 'GYRO_Z']\n",
    "\n",
    "data = raw_data[features]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b043ff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b41bb7e-d505-40a4-9852-b61f04faadfc",
   "metadata": {},
   "source": [
    "**Note**: Some of you might notice that this is a really simple dataset: some of the input data (like `GYRO_*` and `ACC_*`) do not change much over time. Such a dataset is not very challenging and a few, well-placed, thresholds might be sufficient to spot anomalous behaviour. For this tutorial we decided to keep things simple and easy to replicate. Anomalies can be simply triggered by moving the Raspberry Pi around.\n",
    "\n",
    "Keep in mind that this approach is generic: any dataset from any appliance/connected device can be processed in the same way we're showing here. That's the magic of neural networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c893251c",
   "metadata": {},
   "source": [
    "#### Feature scaling\n",
    "\n",
    "AI models don't perform well when the input numerical attributes have very different scales. As you can see `ACC_X`, `ACC_Y` and `ACC_Z` range from 0 to 1, while the `PRESSURE` have far higher values.\n",
    "\n",
    "There are two common ways to address this: _normalization_ and _standardization_.\n",
    "\n",
    "_Normalization_ (a.k.a. Min-max scaling) shifts and rescales values so that they end up ranging from 0 to 1. This can be done by subtracting the min value and dividing by the max minus the min.\n",
    "\n",
    "x' = $\\frac{x - min(x)}{max(x) - min(x)}$\n",
    "\n",
    "_Standardization_ makes the values of each feature in the data have zero-mean (when subtracting the mean in the numerator) and unit-variance. The general method of calculation is to determine the distribution mean and standard deviation for each feature. Next we subtract the mean from each feature. Then we divide the values (mean is already subtracted) of each feature by its standard deviation.\n",
    "\n",
    "x' = $\\frac{x - avg(x)}{\\sigma}$\n",
    "\n",
    "Fortunately for us [scikit-learn](https://scikit-learn.org/stable/) library provides a function for both of them. In this case we'll use _normalization_ because it works well for this application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ed7d19-2bc9-4b7f-a241-695d205efeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data used in the Triton preprocessor\")\n",
    "print(\"-----------Min-----------\")\n",
    "print(data.min())\n",
    "print(\"-----------Max-----------\")\n",
    "print(data.max())\n",
    "print(\"-------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfcd859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a5713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scaled_data).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c527f1",
   "metadata": {},
   "source": [
    "#### Train test split\n",
    "\n",
    "The only way to know how well a model will generalize to new data points is to try it on new data. To do so we split our data into two sets: the training set and the test set.\n",
    "\n",
    "To do so we'll use a function from `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb2614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "x_train, x_test = train_test_split(scaled_data, test_size=0.3, random_state=42)\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f45b25",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d817ad",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "We can now leverage the [Keras](https://keras.io/) API of [Tensorflow](https://www.tensorflow.org/) for creating our Autoencoder and then train it on our dataset.\n",
    "\n",
    "We'll design a neural network architecture such that we impose a bottleneck in the network which forces a compressed knowledge representation of the original input (also called the _latent-space representation_). If the input features were each independent of one another, this compression and subsequent reconstruction would be a very difficult task. However, if some sort of structure exists in the data (ie. correlations between input features), this structure can be learned and consequently leveraged when forcing the input through the network's bottleneck.\n",
    "\n",
    "The bottleneck consists of reducing the number of neurons for each layer of the neural network up to a certain point, and then increase the number until the original input number is reached. This will result in a hourglass shape which is typical for the Autoencoders.\n",
    "\n",
    "![image16.png](imgs/img16.png)\n",
    "\n",
    "#### Build the Autoencoder model\n",
    "\n",
    "In this example we'll use a basic fully-connected autoencoder but keep in mind that autoencoders can be built with different classes of neural network (i.e. Convolutional Neural Networks, Recurrent Neural Networks etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2966fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2' # Avoid AVX2 error\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "\n",
    "def create_model(input_dim):\n",
    "    # The encoder will consist of a number of dense layers that decrease in size\n",
    "    # as we taper down towards the bottleneck of the network, the latent space\n",
    "    input_data = Input(shape=(input_dim,), name='INPUT0')\n",
    "\n",
    "    # hidden layers\n",
    "    encoder = Dense(9, activation='tanh', name='encoder_1')(input_data)\n",
    "    encoder = Dropout(.15)(encoder)\n",
    "    encoder = Dense(6, activation='tanh', name='encoder_2')(encoder)\n",
    "    encoder = Dropout(.15)(encoder)\n",
    "\n",
    "    # bottleneck layer\n",
    "    latent_encoding = Dense(3, activation='linear', name='latent_encoding')(encoder)\n",
    "\n",
    "    # The decoder network is a mirror image of the encoder network\n",
    "    decoder = Dense(6, activation='tanh', name='decoder_1')(latent_encoding)\n",
    "    decoder = Dropout(.15)(decoder)\n",
    "    decoder = Dense(9, activation='tanh', name='decoder_2')(decoder)\n",
    "    decoder = Dropout(.15)(decoder)\n",
    "\n",
    "    # The output is the same dimension as the input data we are reconstructing\n",
    "    reconstructed_data = Dense(input_dim, activation='linear', name='OUTPUT0')(decoder)\n",
    "\n",
    "    autoencoder_model = Model(input_data, reconstructed_data)\n",
    "\n",
    "    return autoencoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a63862",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_model = create_model(len(features))\n",
    "autoencoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6357e8-8bab-4fba-a8fc-4e861c7a11eb",
   "metadata": {},
   "source": [
    "![image17.png](imgs/img17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9105a68",
   "metadata": {},
   "source": [
    "#### Model training\n",
    "\n",
    "As we already explained, the autoencoder is a  type of artificial neural network used to learn efficient codings of unlabeled data. We'll use that to reconstruct the input at the output. To train an autoencoder we don’t need to do anything fancy, just throw the raw input data at it. Autoencoders are considered an unsupervised learning technique since they don’t need explicit labels to train on but to be more precise they are self-supervised because they generate their own labels from the training data.\n",
    "\n",
    "To train our neural network we need to have a performance metric to measure how well it is learning to reconstruct the data i.e. our _loss function_. The loss function in our example, which we need to minimize during our training, is the error between the _input data_ and the _data reconstructed by the autoencoder_. We'll use the [Mean Squared Error](https://en.wikipedia.org/wiki/Mean_squared_error).\n",
    "\n",
    "MSE = $\\frac{1}{n}\\sum_{i=1}^{n}{(Y_i - Y'_i)^2}$\n",
    "\n",
    "Where:\n",
    "- $n$: is the number of features (10 in our example)\n",
    "- $Y_i$: is the original data point i.e. the input of the autoencoder \n",
    "- $Y'_i$: is the reconstructed data point i.e. the output of the autoencoder\n",
    "\n",
    "Before starting the training we need to set the [**hyperparameters**](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)). Hyperparameters are parameters whose values control the learning process and determine the values of model parameters that a learning algorithm ends up learning. These are the `learning_rate`, `max_epochs`, `optimizer` and the `batch_size` you see in the code snippet below. You may ask yourself how to set them, it all comes down to trial and error. Try tweaking them below and see how they affect the learning process...\n",
    "\n",
    "A good explaination of their meaning can be found in the [Keras documentation](https://keras.io/api/models/model_training_apis/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0946be96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "batch_size = 32\n",
    "max_epochs = 15\n",
    "learning_rate = .0001\n",
    "\n",
    "opt = optimizers.Adam(learning_rate=learning_rate)\n",
    "autoencoder_model.compile(optimizer=opt, loss='mse', metrics=['accuracy'])\n",
    "train_history = autoencoder_model.fit(x_train, x_train,\n",
    "                      shuffle=True,\n",
    "                      epochs=max_epochs,\n",
    "                      batch_size=batch_size,\n",
    "                      validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74af6d4d-c65c-4e4e-a811-28ff05f94b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_history.history['loss'])\n",
    "plt.plot(train_history.history['val_loss'])\n",
    "plt.legend(['loss on train data', 'loss on test data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd73b5c0-dd8c-4238-8d24-cdf9df30b12e",
   "metadata": {},
   "source": [
    "Here we can see the loss for the training set and the test set on the epochs.\n",
    "\n",
    "Some of you might notice that this graph is somewhat unexpected. Why the validation loss is lower than the train loss? This is the effect of the regularization: regularization terms and dropout layer are affecting the network during training. A good writeup of this effect can be found [here](https://towardsdatascience.com/what-your-validation-loss-is-lower-than-your-training-loss-this-is-why-5e92e0b1747e).\n",
    "\n",
    "As an excercise try and compute the average MSE on the training set and the test set. You'll find that the MSE is lower in the training set!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4a8528",
   "metadata": {},
   "source": [
    "We can now save the model on disk as we'll use this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83811ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_model.save(\"./saved_model/autoencoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba0be49",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./saved_model/autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff7d005",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad6601",
   "metadata": {},
   "source": [
    "### Model evaluation\n",
    "\n",
    "We now have a model that reconstruct the input at the output... doesn't sounds really useful right?\n",
    "\n",
    "Let's see it in action. Let's take a sample from the test set and run it through our autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a9d53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sample = x_test[3:4].copy() # Deep copy\n",
    "\n",
    "reconstructed_sample = autoencoder_model.predict(input_sample)\n",
    "\n",
    "print(input_sample)\n",
    "print(reconstructed_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb88933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "index = np.arange(10)\n",
    "bar_width = 0.35\n",
    "\n",
    "figure, ax = plt.subplots()\n",
    "\n",
    "inbar = ax.bar(index, input_sample[0], bar_width, label=\"Input data\")\n",
    "recbar = ax.bar(index+bar_width, reconstructed_sample[0], bar_width, label=\"Reconstruced data\")\n",
    "\n",
    "ax.set_xlabel('Features')\n",
    "ax.set_xticks(index + bar_width / 2)\n",
    "ax.set_xticklabels(features, rotation = 45)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415bc7dd",
   "metadata": {},
   "source": [
    "As we can see from the graph above it reconstructed the input fairly well. It is not perfect since the Autoencoder is lossy but it is good enough\n",
    "\n",
    "What happens if we manipulate this sample in a way the autoencoder doesn't expect (i.e. we introduce an **anomaly**)?\n",
    "\n",
    "Let's try and set the `ACC_Z` to a value the autoencoder has never seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786d3509",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_anomaly = input_sample.copy() # Deep copy\n",
    "\n",
    "input_anomaly[0][2] = 0.15\n",
    "\n",
    "reconstructed_anomaly = autoencoder_model.predict(input_anomaly)\n",
    "\n",
    "print(input_anomaly)\n",
    "print(reconstructed_anomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77393430",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots()\n",
    "\n",
    "inbar = ax.bar(index, input_anomaly[0], bar_width, label=\"Input anomaly\")\n",
    "recbar = ax.bar(index+bar_width, reconstructed_anomaly[0], bar_width, label=\"Reconstruced anomaly\")\n",
    "\n",
    "ax.set_xlabel('Features')\n",
    "ax.set_xticks(index + bar_width / 2)\n",
    "ax.set_xticklabels(features, rotation = 45)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06166d2a",
   "metadata": {},
   "source": [
    "The autoencoder fails to reconstruct the data it received at the input. This means that the reconstruction error is very high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd374df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(\"Anomaly %f\"% mean_squared_error(input_anomaly[0], reconstructed_anomaly[0]))\n",
    "print(\"Normal  %f\"% mean_squared_error(input_sample[0], reconstructed_sample[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d3391f",
   "metadata": {},
   "source": [
    "**It's working as expected!**\n",
    "\n",
    "We now need to decide when to trigger an alarm (i.e. classify an input sample as anomalous) from this reconstruction error. In other words we need to decide our threshold.\n",
    "\n",
    "There are multiple ways to set this value, in this example we'll use the [Z-Score](https://en.wikipedia.org/wiki/Standard_score).\n",
    "\n",
    "From Wikipedia:\n",
    "> In statistics, the standard score is the number of standard deviations by which the value of a raw score (i.e., an observed value or data point) is above or below the mean value of what is being observed or measured.[...]\n",
    ">\n",
    "> It is calculated by subtracting the population mean from an individual raw score and then dividing the difference by the population standard deviation.\n",
    "\n",
    "We'll consider a sample an anomaly if the Reconstruction Error Z-Score is not in the range \\[-2, +2\\]. This means that if the reconstruction error for a sample is more than 2 standard deviation away from the average reconstruction error computed on the test set, the sample is an anomaly. This choice is arbirtary, we can control the sensitivity of the detector by changing this range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322bd21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_recon = autoencoder_model.predict(x_test)\n",
    "reconstruction_scores = np.mean((x_test - x_test_recon)**2, axis=1)  # MSE\n",
    "\n",
    "reconstruction_scores_pd = pd.DataFrame({'recon_score': reconstruction_scores})\n",
    "print(reconstruction_scores_pd.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37738a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(mse_sample):\n",
    "    return (mse_sample - reconstruction_scores_pd.mean())/reconstruction_scores_pd.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0bb404-ad11-4438-8832-55e087cc7fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_anomaly = mean_squared_error(input_anomaly[0], reconstructed_anomaly[0])\n",
    "mse_normal = mean_squared_error(input_sample[0], reconstructed_sample[0])\n",
    "\n",
    "z_score_anomaly = z_score(mse_anomaly)\n",
    "z_score_normal = z_score(mse_normal)\n",
    "\n",
    "print(\"Anomaly Z-score %f\"% z_score_anomaly)\n",
    "print(\"Normal Z-score %f\"% z_score_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eb17c8",
   "metadata": {},
   "source": [
    "We now have our anomaly detector... let's see how we can deploy it on our Kura&trade;-powered edge device."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e27579",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460cd4a4",
   "metadata": {},
   "source": [
    "## Model deployment\n",
    "\n",
    "To deploy our model on the target device we'll leverage Kura&trade;'s newly added [Nvidia&trade; Triton Inferece Server](https://developer.nvidia.com/nvidia-triton-inference-server) integration.\n",
    "\n",
    "![image18.png](imgs/img18.png)\n",
    "\n",
    "The Nvidia™ Triton Inference Server is an open-source inference service software that enables the user to deploy trained AI models from any framework on GPU or CPU infrastructure. It supports all major frameworks like TensorFlow, TensorRT, PyTorch, ONNX Runtime, and even custom framework backend. With specific backends, it is also possible to run Python scripts, mainly for pre-and post-processing purposes, and exploit the DALI building block for optimized operations.\n",
    "\n",
    "For installation refer to the official [Kura&trade;](https://eclipse.github.io/kura/docs-develop/core-services/nvidia-triton-server-inference-engine/#nvidiatm-triton-server-installation) and [Triton documentation](https://github.com/triton-inference-server/server/tree/main/docs#installation). For the rest of this tutorial we'll assume a Triton container is available on the target device. It can be simply installed with:\n",
    "\n",
    "```bash\n",
    "docker pull nvcr.io/nvidia/tritonserver:22.07-tf2-python-py3\n",
    "```\n",
    "\n",
    "We'll also need to install Kura&trade;'s Triton bundles:\n",
    "- [Triton Server Component](https://marketplace.eclipse.org/content/triton-server-component-eclipse-kura-5): for Kura-Triton integration\n",
    "- [AI Wire Component](https://marketplace.eclipse.org/content/ai-wire-component-eclipse-kura-5): for making the Triton Inference Server available through the Kura Wires as a Wire component.\n",
    "\n",
    "### Model conversion\n",
    "\n",
    "The first step in using Triton to serve your models is to place one or more models into a [model repository](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_repository.md) i.e. a folder were the model are available for Triton to load. Depending on the type of the model and on what Triton capabilities you want to enable for the model, you may need to create a [model configuration](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md) for the model. This configuration is a protobuf containing informations about runtime configuration and input/output shape accepted by the model.\n",
    "\n",
    "For our autoencoder model we'll need three \"models\":\n",
    "- A **Preprocessor** for performing the operations described in the [\"Data processing\"](#Data-Processing) section (Wire envelop translation, feature selection and scaling)\n",
    "- The **Autoencoder** model we exported in the [\"Model training\"](#Model-training) section\n",
    "- A **Postprocessor** for performing the operations described in the [\"Model evaluation\"](#Model-evaluation) section (Reconstruction error computation)\n",
    "\n",
    "To simplify the handling of these models and improve inference performance, we'll use an advanced feature of Triton wich is an [Ensemble Model](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/architecture.md#ensemble-models). From Triton official documentation:\n",
    "\n",
    "> An ensemble model represents a pipeline of one or more models and the connection of input and output tensors between those models. Ensemble models are intended to be used to encapsulate a procedure that involves multiple models, such as \"data preprocessing -> inference -> data postprocessing\". Using ensemble models for this purpose can avoid the overhead of transferring intermediate tensors and minimize the number of requests that must be sent to Triton."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030b5de3-0dcf-44b3-a1e6-58ee551e216d",
   "metadata": {},
   "source": [
    "#### Autoencoder\n",
    "\n",
    "As seen in the [\"Model training\"](#Model-training) section, our model is available as a [Tensorflow _SavedModel_](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_repository.md#tensorflow-models) which can be simply loaded by the Triton [Tensorflow backend](https://github.com/triton-inference-server/tensorflow_backend). We just need to configure it properly.\n",
    "\n",
    "We'll start by creating the following folder structure\n",
    "\n",
    "```\n",
    "tf_autoencoder_fp32\n",
    "├── 1\n",
    "│   └── model.savedmodel\n",
    "│       ├── assets\n",
    "│       ├── keras_metadata.pb\n",
    "│       ├── saved_model.pb\n",
    "│       └── variables\n",
    "│           ├── variables.data-00000-of-00001\n",
    "│           └── variables.index\n",
    "└── config.pbtxt\n",
    "```\n",
    "\n",
    "This can be done by copying the model we saved in the Model Training section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba56fa55-6332-447d-bbbe-4d76ab46202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./tf_autoencoder_fp32/ && mkdir -p ./tf_autoencoder_fp32/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eeeb5e-2959-4df6-b7ed-32b8d305fdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfc13c4-40b3-4fb4-b0ae-c5df800d06e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp -r ./saved_model/autoencoder tf_autoencoder_fp32/1/model.savedmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdebc28-0337-4a52-bdd6-0a900a13de3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree tf_autoencoder_fp32/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e03e71-9e68-4ef0-a692-e1724180f67a",
   "metadata": {},
   "source": [
    "Now comes the hard part: we need to provide the [model configuration](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md) (i.e. the `config.pbtxt` file). In the case of the autoencoder is pretty simple:\n",
    "\n",
    "```protobuf\n",
    "name: \"tf_autoencoder_fp32\"\n",
    "backend: \"tensorflow\"\n",
    "max_batch_size: 0\n",
    "input [\n",
    "    {\n",
    "    name: \"INPUT0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1, 10 ]\n",
    "    }\n",
    "]\n",
    "output [\n",
    "    {\n",
    "    name: \"OUTPUT0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1, 10 ]\n",
    "    }\n",
    "]\n",
    "version_policy: { all { }}\n",
    "instance_group [{ kind: KIND_CPU }]\n",
    "```\n",
    "\n",
    "Each model `input` and `output` must specify the `name`, `data_type` and `dims`. We already know all of these:\n",
    "- `name`: corresponds to the layer name we've seen in the Model Training section. `INPUT0` for the input and `OUTPUT0` for the output.\n",
    "- `data_type`: will be float since we didn't perform any quantization\n",
    "- `dims`: is the shape of the in/out tensor. In this case it will correspond to an array with the same length as the number of features.\n",
    "\n",
    "Other interesting parameters of this configuration are:\n",
    "- `backend`: where we set the backend for the model. In this case it will be the Tensorflow backend\n",
    "- `name`: the name of the model that must correspond to the name of the folder\n",
    "- `instance_group`: where we set where we want the model to run. In this case we'll use the CPU since we're on a Raspberry Pi but keep in mind that Triton support multiple accelerators.\n",
    "\n",
    "for a deep dive into the model configuration parameter take a look at the [official documentation](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546893f3-02c3-46f1-822a-2af4bd2cdd40",
   "metadata": {},
   "source": [
    "#### Preprocessor\n",
    "\n",
    "As discussed in the [\"Data processing\"](#Data-Processing) section, before providing the incoming data to the autoencoder, we need to perform feature selection and scaling. In addition to these responsibilites, the Preprocessor will need to perform a sort of serialization of the data to comply to the input shape accepted by the Autoencoder. This is due to how Kura manages the data running on Wires. More details can be found [here](https://eclipse.github.io/kura/docs-develop/kura-wires/single-port-wire-components/ai-wire-component/#models-input-and-output-formats).\n",
    "\n",
    "To perform all of this we'll use the [Python backend](https://github.com/triton-inference-server/python_backend) available in Triton.\n",
    "\n",
    "As described in the previous section we will need to provide the following folder structure:\n",
    "\n",
    "```\n",
    "preprocessor\n",
    "├── 1\n",
    "│   └── model.py\n",
    "└── config.pbtxt\n",
    "```\n",
    "\n",
    "##### Preprocessor Configuration\n",
    "\n",
    "As discussed in the [official Kura documentation](https://eclipse.github.io/kura/docs-develop/kura-wires/single-port-wire-components/ai-wire-component/#models-input-and-output-formats):\n",
    "\n",
    "> The AI wire component takes a WireEnvelope as an input, it processes its records and feeds them to the specified preprocessing or inference model.\n",
    ">\n",
    "> ...\n",
    ">\n",
    "> The models that manage the input and the output must expect a list of inputs such that:\n",
    "> - each input corresponds to an entry of the `WireRecord` properties\n",
    "> - the entry key will become the input name (e.g. in the case of an asset, the channel name becomes the tensor name)\n",
    "> - input shape will be `[1]`\n",
    "\n",
    "Therefore for our `input` we'll have that each name corresponds to the names we've seen in the Data Collection section. The `output` needs to correspond to the input accepted by the model (i.e. `INPUT0`).\n",
    "\n",
    "```protobuf\n",
    "name: \"preprocessor\"\n",
    "backend: \"python\"\n",
    "\n",
    "input [\n",
    "  {\n",
    "    name: \"ACC_X\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1 ]\n",
    "  }\n",
    "]\n",
    "input [\n",
    "  {\n",
    "    name: \"ACC_Y\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1 ]\n",
    "  }\n",
    "]\n",
    " ...\n",
    "input [\n",
    "  {\n",
    "    name: \"TEMP_PRESS\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"INPUT0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1, 10 ]\n",
    "  }\n",
    "]\n",
    "instance_group [{ kind: KIND_CPU }]\n",
    "```\n",
    "\n",
    "##### Preprocessor Model\n",
    "\n",
    "As we've seen in the Data Processing section the Preprocessor is responsible for scaling the input features and serializing them in the tensor shape expected by the Autoencoder model.\n",
    "\n",
    "This can be done with the following python script:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import triton_python_backend_utils as pb_utils\n",
    "\n",
    "\n",
    "class TritonPythonModel:\n",
    "\n",
    "    def initialize(self, args):\n",
    "        self.model_config = model_config = json.loads(args['model_config'])\n",
    "\n",
    "        output0_config = pb_utils.get_output_config_by_name(\n",
    "            model_config, \"INPUT0\")\n",
    "\n",
    "        self.output0_dtype = pb_utils.triton_string_to_numpy(\n",
    "            output0_config['data_type'])\n",
    "\n",
    "    def execute(self, requests):\n",
    "        output0_dtype = self.output0_dtype\n",
    "\n",
    "        responses = []\n",
    "\n",
    "        for request in requests:\n",
    "            acc_x      = pb_utils.get_input_tensor_by_name(request, \"ACC_X\").as_numpy()\n",
    "            acc_y      = pb_utils.get_input_tensor_by_name(request, \"ACC_Y\").as_numpy()\n",
    "            acc_z      = pb_utils.get_input_tensor_by_name(request, \"ACC_Z\").as_numpy()\n",
    "            gyro_x     = pb_utils.get_input_tensor_by_name(request, \"GYRO_X\").as_numpy()\n",
    "            gyro_y     = pb_utils.get_input_tensor_by_name(request, \"GYRO_Y\").as_numpy()\n",
    "            gyro_z     = pb_utils.get_input_tensor_by_name(request, \"GYRO_Z\").as_numpy()\n",
    "            humidity   = pb_utils.get_input_tensor_by_name(request, \"HUMIDITY\").as_numpy()\n",
    "            pressure   = pb_utils.get_input_tensor_by_name(request, \"PRESSURE\").as_numpy()\n",
    "            temp_hum   = pb_utils.get_input_tensor_by_name(request, \"TEMP_HUM\").as_numpy()\n",
    "            temp_press = pb_utils.get_input_tensor_by_name(request, \"TEMP_PRESS\").as_numpy()\n",
    "\n",
    "            out_0 = np.array([acc_y, acc_x, acc_z, pressure, temp_press, temp_hum, humidity, gyro_x, gyro_y, gyro_z]).transpose()\n",
    "\n",
    "            #                  ACC_Y     ACC_X     ACC_Z    PRESSURE   TEMP_PRESS   TEMP_HUM   HUMIDITY    GYRO_X    GYRO_Y    GYRO_Z\n",
    "            min = np.array([-0.132551, -0.049693, 0.759847, 976.001709, 38.724998, 40.220890, 13.003981, -1.937896, -0.265019, -0.250647])\n",
    "            max = np.array([ 0.093099, 0.150289, 1.177543, 1007.996338, 46.093750, 48.355824, 23.506138, 1.923712, 0.219204, 0.671759])\n",
    "\n",
    "            # MinMax scaling\n",
    "            out_0_scaled = (out_0 - min)/(max - min)\n",
    "\n",
    "            # Create output tensor\n",
    "            out_tensor_0 = pb_utils.Tensor(\"INPUT0\",\n",
    "                                           out_0_scaled.astype(output0_dtype))\n",
    "\n",
    "            inference_response = pb_utils.InferenceResponse(\n",
    "                output_tensors=[out_tensor_0])\n",
    "            responses.append(inference_response)\n",
    "\n",
    "        return responses\n",
    "```\n",
    "\n",
    "Here there are two important things to note:\n",
    "- The template we're using is taken from the Triton documentation and can be found [here](https://github.com/triton-inference-server/python_backend/blob/main/examples/add_sub/model.py).\n",
    "- The MinMax scaling **must be the same we used in our training**. For illustration purposes we wrote the `min` and `max` arrays we found in the Data Processing section but we could have serialized the `MinMaxScaler` using [`pickle`](https://docs.python.org/3/library/pickle.html) instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa10670-93d7-4f7d-98c3-8a3de21c38fa",
   "metadata": {},
   "source": [
    "#### Postprocessor\n",
    "\n",
    "As discussed in the [\"Data processing\"](#Data-Processing) section, to perform the anomaly detection step we need to compute the Mean Squared Error between the recontructed data and the actual input data. Due to this the configuration of the Postprocessor model will be somewhat more complicated than before: in addition to the output of the Autoencoder model we will need the output of the Preprocessor model.\n",
    "\n",
    "To perform all of this we'll use the [Python backend](https://github.com/triton-inference-server/python_backend) again.\n",
    "\n",
    "As described in the previous section we will need to provide the following folder structure:\n",
    "\n",
    "```\n",
    "postprocessor\n",
    "├── 1\n",
    "│   └── model.py\n",
    "└── config.pbtxt\n",
    "```\n",
    "\n",
    "##### Postprocessor Configuration\n",
    "\n",
    "```protobuf\n",
    "name: \"postprocessor\"\n",
    "backend: \"python\"\n",
    "\n",
    "input [\n",
    "  {\n",
    "    name: \"RECONSTR0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1, 10 ]\n",
    "  }\n",
    "]\n",
    "input [\n",
    "  {\n",
    "    name: \"ORIG0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1, 10 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"ANOMALY_SCORE0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"ANOMALY0\"\n",
    "    data_type: TYPE_BOOL\n",
    "    dims: [ 1 ]\n",
    "  }\n",
    "]\n",
    "instance_group [{ kind: KIND_CPU }]\n",
    "```\n",
    "\n",
    "As we can see we have two inputs and two outputs:\n",
    "- The first input tensor is the reconstruction performed by the autoencoder model\n",
    "- The second input tensor is the original data (already scaled and serialized by the Preprocessor model)\n",
    "- The first output is the anomaly score i.e. the reconstruction error between the original and the reconstructed data.\n",
    "- The second output is a boolean representing whether the data constitute an anomaly or not\n",
    "\n",
    "Let's see how this is computed by the Python model.\n",
    "\n",
    "##### Postprocessor Model\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import triton_python_backend_utils as pb_utils\n",
    "\n",
    "def z_score(mse):\n",
    "    return (mse - MEAN_MSE)/STD_MSE\n",
    "\n",
    "\n",
    "class TritonPythonModel:\n",
    "\n",
    "    def initialize(self, args):\n",
    "        self.model_config = model_config = json.loads(args['model_config'])\n",
    "\n",
    "        output0_config = pb_utils.get_output_config_by_name(\n",
    "            model_config, \"ANOMALY_SCORE0\")\n",
    "        output1_config = pb_utils.get_output_config_by_name(\n",
    "            model_config, \"ANOMALY0\")\n",
    "\n",
    "        self.output0_dtype = pb_utils.triton_string_to_numpy(\n",
    "            output0_config['data_type'])\n",
    "        self.output1_dtype = pb_utils.triton_string_to_numpy(\n",
    "            output1_config['data_type'])\n",
    "\n",
    "    def execute(self, requests):\n",
    "        output0_dtype = self.output0_dtype\n",
    "        output1_dtype = self.output1_dtype\n",
    "\n",
    "        responses = []\n",
    "\n",
    "        for request in requests:\n",
    "            # Get input\n",
    "            x_recon = pb_utils.get_input_tensor_by_name(request, \"RECONSTR0\").as_numpy()\n",
    "            x_orig = pb_utils.get_input_tensor_by_name(request, \"ORIG0\").as_numpy()\n",
    "\n",
    "            # Get Mean square error between reconstructed input and original input\n",
    "            reconstruction_score = np.mean((x_orig - x_recon)**2, axis=1)\n",
    "            \n",
    "            # Z-Score of Mean square error must be inside [-2; 2]\n",
    "            anomaly = np.array([z_score(reconstruction_score) < -2.0 or z_score(reconstruction_score) > 2.0])\n",
    "\n",
    "            # Create output tensors\n",
    "            out_tensor_0 = pb_utils.Tensor(\"ANOMALY_SCORE0\",\n",
    "                                           reconstruction_score.astype(output0_dtype))\n",
    "            out_tensor_1 = pb_utils.Tensor(\"ANOMALY0\",\n",
    "                                           anomaly.astype(output1_dtype))\n",
    "\n",
    "            inference_response = pb_utils.InferenceResponse(\n",
    "                output_tensors=[out_tensor_0, out_tensor_1])\n",
    "            responses.append(inference_response)\n",
    "\n",
    "        return responses\n",
    "```\n",
    "\n",
    "As you can see the script is simple:\n",
    "- It gets the input tensors\n",
    "- It computes the Mean Squared Error between the inputs (which is what we called the reconstruction error)\n",
    "- It computes the Z-Score of the MSE computed for the current sample and flags it as an anomaly if it is farther than 2 standard deviations away from the average MSE.\n",
    "\n",
    "**Note**: `MEAN_MSE` and `STD_MSE` are the mean value and the standard deviation of the Mean Squared Error computed on the test set and correspond to the `reconstruction_scores_pd.mean()` and `reconstruction_scores_pd.std()` we used in the previous section. We didn't set them as they change for every training performed on the Autoencoder. Be sure to set it to their proper values before trying this model on the Triton server!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5715b658-49d8-42ba-b93e-14580cb0d598",
   "metadata": {},
   "source": [
    "### Ensemble model\n",
    "\n",
    "To make things easier for ourselves and improve performance we'll consolidate the AI pipeline into an [Ensemble Model](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/architecture.md#ensemble-models).\n",
    "\n",
    "We will need to provide the following folder structure:\n",
    "\n",
    "```\n",
    "ensemble_pipeline\n",
    "├── 1\n",
    "└── config.pbtxt\n",
    "```\n",
    "\n",
    "Note that the `1` folder is **empty**. The ensemble model essentially describe *how to connect the models that belong to the processing pipeline*.\n",
    "\n",
    "Therefore we'll need to focus on the configuration only.\n",
    "\n",
    "```protobuf\n",
    "name: \"ensemble_pipeline\"\n",
    "platform: \"ensemble\"\n",
    "max_batch_size: 0\n",
    "input [\n",
    "  {\n",
    "    name: \"ACC_X\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1 ]\n",
    "  }\n",
    "]\n",
    "input [\n",
    "  {\n",
    "    name: \"ACC_Y\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1 ]\n",
    "  }\n",
    "]\n",
    " ...\n",
    "input [\n",
    "  {\n",
    "    name: \"TEMP_PRESS\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"ANOMALY_SCORE0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"ANOMALY0\"\n",
    "    data_type: TYPE_BOOL\n",
    "    dims: [ 1 ]\n",
    "  }\n",
    "]\n",
    "ensemble_scheduling {\n",
    "  step [\n",
    "    {\n",
    "      model_name: \"preprocessor\"\n",
    "      model_version: -1\n",
    "      input_map{\n",
    "          key: \"ACC_X\"\n",
    "          value: \"ACC_X\"\n",
    "      }\n",
    "      input_map{\n",
    "          key: \"ACC_Y\"\n",
    "          value: \"ACC_Y\"\n",
    "      }\n",
    "       ...\n",
    "      input_map{\n",
    "          key: \"TEMP_PRESS\"\n",
    "          value: \"TEMP_PRESS\"\n",
    "      }\n",
    "      output_map {\n",
    "        key: \"INPUT0\"\n",
    "        value: \"preprocess_out\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      model_name: \"tf_autoencoder_fp32\"\n",
    "      model_version: -1\n",
    "      input_map {\n",
    "        key: \"INPUT0\"\n",
    "        value: \"preprocess_out\"\n",
    "      }\n",
    "      output_map {\n",
    "        key: \"OUTPUT0\"\n",
    "        value: \"autoencoder_output\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      model_name: \"postprocessor\"\n",
    "      model_version: -1\n",
    "      input_map {\n",
    "        key: \"RECONSTR0\"\n",
    "        value: \"autoencoder_output\"\n",
    "      }\n",
    "      input_map {\n",
    "        key: \"ORIG0\"\n",
    "        value: \"preprocess_out\"\n",
    "      }\n",
    "      output_map {\n",
    "        key: \"ANOMALY_SCORE0\"\n",
    "        value: \"ANOMALY_SCORE0\"\n",
    "      }\n",
    "      output_map {\n",
    "        key: \"ANOMALY0\"\n",
    "        value: \"ANOMALY0\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "The configuration is split in two main parts:\n",
    "- The first is the usual configuration we've seen before: we describe what are the input and the output of our model. In this case the input will correspond to the input of the first model of the pipeline (the Preprocessor) and the output to the output of the last model of the pipeline (the Postprocessor)\n",
    "- The second part describe how to map the input/output of the models within the pipeline\n",
    "\n",
    "To better visualize the configuration we can look at the graph below.\n",
    "\n",
    "![image19.png](imgs/img19.png)\n",
    "\n",
    "#### Conversion results\n",
    "\n",
    "At this point we should have a folder structure that looks like this:\n",
    "\n",
    "```\n",
    "models\n",
    "├── ensemble_pipeline\n",
    "│   ├── 1\n",
    "│   └── config.pbtxt\n",
    "├── postprocessor\n",
    "│   ├── 1\n",
    "│   │   └── model.py\n",
    "│   └── config.pbtxt\n",
    "├── preprocessor\n",
    "│   ├── 1\n",
    "│   │   └── model.py\n",
    "│   └── config.pbtxt\n",
    "└── tf_autoencoder_fp32\n",
    "    ├── 1\n",
    "    │   └── model.savedmodel\n",
    "    │       ├── assets\n",
    "    │       ├── keras_metadata.pb\n",
    "    │       ├── saved_model.pb\n",
    "    │       └── variables\n",
    "    │           ├── variables.data-00000-of-00001\n",
    "    │           └── variables.index\n",
    "    └── config.pbtxt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8f07cb-2559-4609-a57f-bf6ffaeb313d",
   "metadata": {},
   "source": [
    "### Kura Deployment\n",
    "\n",
    "We can now move our pipeline to the target device for inference on the edge.\n",
    "\n",
    "We want to perform anomaly detection in real time, directly within the edge device, using the same data we used to collect for our training.\n",
    "\n",
    "![image20.png](imgs/img20.png)\n",
    "\n",
    "#### Triton component configuration\n",
    "\n",
    "To do so we need to copy the `models` folder on the target device. For this example we'll use the `/home/pi/models` path.\n",
    "\n",
    "We can now move to the Kura web UI and create a new Triton Server Container Service component instance. The complete documentation can be found [here](https://eclipse.github.io/kura/docs-release-5.2/core-services/nvidia-triton-server-inference-engine/).\n",
    "\n",
    "In this example we'll call it `TritonContainerService`.\n",
    "\n",
    "![image21.png](imgs/img21.png)\n",
    "\n",
    "Then we'll need to configure it to run our models. Move to the `TritonContainerService` configuration interface and set the following parameters:\n",
    "\n",
    "- **Image name**/**Image tag**: use the name and tag of the Triton container image you installed. We're using `nvcr.io/nvidia/tritonserver:22.07-tf2-python-py3` in this example.\n",
    "- **Local model repository path**: in our example is `/home/pi/models`\n",
    "- **Inference Models**: we'll need to load all the models of the pipeline so: `preprocessor,postprocessor,tf_autoencoder_fp32,ensemble_pipeline`\n",
    "- **Optional configuration for the local backends**: `tensorflow,version=2` since Tensorflow 2 is the only available Tensorflow backend in the Triton container image we're using.\n",
    "\n",
    "You can leave everything else as default.\n",
    "\n",
    "![image22.png](imgs/img22.png)\n",
    "\n",
    "Once you press the \"_Apply_\" button Kura will create a new container from the Triton image we set and spin up the service with our models loaded.\n",
    "\n",
    "```bash\n",
    "pi@raspberrypi:~ $ docker ps\n",
    "CONTAINER ID   IMAGE                                              COMMAND                  CREATED          STATUS          PORTS                                                                                                                             NAMES\n",
    "4deae2857b6f   nvcr.io/nvidia/tritonserver:22.07-tf2-python-py3   \"tritonserver --mode…\"   13 seconds ago   Up 11 seconds   0.0.0.0:4000->8000/tcp, :::4000->8000/tcp, 0.0.0.0:4001->8001/tcp, :::4001->8001/tcp, 0.0.0.0:4002->8002/tcp, :::4002->8002/tcp   tritonserver-kura\n",
    "```\n",
    "\n",
    "> **Note**: if no container is created check that the \"_Container Orchestration Service_\" is enabled in the Kura UI. Full documentation for the service can be found [here](https://eclipse.github.io/kura/docs-release-5.2/core-services/container-orchestration-provider-usage/#starting-the-service).\n",
    "\n",
    "> **Note**: if you see an error in the logs like \"_Internal: Unable to initialize shared memory key 'triton_python_backend_shm_region_2' to requested size (67108864 bytes). If you are running Triton inside docker, use '--shm-size' flag to control the shared memory region size. Each Python backend model instance requires at least 64MBs of shared memory._\", you can update the default shared memory size allocated by the Docker daemon. Go to `/etc/docker/daemon.json`, set `\"default-shm-size\": \"200m\"` and restart the Docker daemon with: `sudo systemctl restart docker`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3732a18-d43e-4cec-895f-29533faafbc3",
   "metadata": {},
   "source": [
    "#### Wire Graph\n",
    "\n",
    "Finally we can move to the \"_Wire Graph_\" UI and create the AI component (in the Emitters/Receiver menu) for interfacing with the Triton instance we just created. We'll call it `Triton` in this example.\n",
    "\n",
    "![image23.png](imgs/img23.png)\n",
    "\n",
    "We just need to change two parameter in the configuration:\n",
    "- **InferenceEngineService Target Filter**: we need to select the `TritonContainerService` we created at the step above\n",
    "- **inference.model.name**: Since we're using an ensemble pipeline we need only that as our inference model.\n",
    "\n",
    "The resulting wire graph is the following:\n",
    "\n",
    "![image24.png](imgs/img24.png)\n",
    "\n",
    "And that's it! We should now see the anomaly detection results coming to Kapua in addition to the SenseHat data.\n",
    "\n",
    "![image25.png](imgs/img25.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d11ef3-36cb-48ec-9005-43f497e598c3",
   "metadata": {},
   "source": [
    "## Complete Example\n",
    "\n",
    "A similar but more complete example of the feature presented in this notebook is available [in the official Kura&trade; repository](https://github.com/eclipse/kura/tree/develop/kura/examples/scenarios/org.eclipse.kura.example.ai) containing all the code and the configuration needed to make it work.\n",
    "\n",
    "Give it a try!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
